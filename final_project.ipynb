{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0L_iZxR4JEqb",
    "outputId": "b29db25c-f039-42e0-c20b-371af7c47e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\darth\\.cache\\kagglehub\\datasets\\dgawlik\\nyse\\versions\\3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fundamentals.csv',\n",
       " 'prices-split-adjusted.csv',\n",
       " 'prices.csv',\n",
       " 'securities.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cisco Dacanay\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dgawlik/nyse\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aXm5j5zSLSaR"
   },
   "outputs": [],
   "source": [
    "fundamentals = pd.read_csv(path + \"/fundamentals.csv\") # SEC annual filing reports, 449 securities\n",
    "prices = pd.read_csv(path + \"/prices-split-adjusted.csv\") # Split-adjusted prices, 502 securities\n",
    "# securities = pd.read_csv(path + \"/securities.csv\") # List of securities/tickers, 506 securities\n",
    "\n",
    "# limit data size for faster testing\n",
    "# fundamentals = fundamentals.iloc[:20].copy()\n",
    "\n",
    "# Filter out securities that aren't in fundamentals data\n",
    "prices = prices[prices[\"symbol\"].isin(fundamentals[\"Ticker Symbol\"])]\n",
    "\n",
    "# Not using securities csv since only the ticker symbol is being used\n",
    "securities = fundamentals[\"Ticker Symbol\"].unique()\n",
    "securities = pd.DataFrame(securities, columns=[\"Ticker symbol\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2v_VKSJ4dgVS"
   },
   "outputs": [],
   "source": [
    "# Convert date strings in dataset to datetime\n",
    "datetime_dates_fundamentals = []\n",
    "for index, date in fundamentals[\"Period Ending\"].items():\n",
    "  date = pd.to_datetime(date)\n",
    "  datetime_dates_fundamentals.append(date)\n",
    "\n",
    "datetime_dates_prices = []\n",
    "for index, date in prices[\"date\"].items():\n",
    "  date = pd.to_datetime(date)\n",
    "  datetime_dates_prices.append(date)\n",
    "\n",
    "# Replace date string columns with datetime lists\n",
    "fundamentals[\"Period Ending\"] = datetime_dates_fundamentals\n",
    "prices[\"date\"] = datetime_dates_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lELZKIweJbpz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1318 rows\n"
     ]
    }
   ],
   "source": [
    "# Input features: 2 consecutive years of fundamentals reports data, price data in between those 2 reports (predicting future price based off ~1 year of price data and 2 SEC filings)\n",
    "\n",
    "price_count = 50 # how many price data points will be given to the models (should be around 250 data points per year)\n",
    "prediction_lead = 22 # how many business days ahead the price will be predicted (22 business days ~ 30 days)\n",
    "\n",
    "feature_data = []\n",
    "label_data = []\n",
    "\n",
    "for ticker in securities[\"Ticker symbol\"]:\n",
    "  # print(ticker)\n",
    "  report_dates = []\n",
    "  report_data = []\n",
    "\n",
    "  # Get period ending dates of reports for each ticker\n",
    "  for index, ticker_reports in fundamentals[fundamentals[\"Ticker Symbol\"] == ticker].iterrows():\n",
    "    # print(ticker_reports[\"Period Ending\"])\n",
    "    report_date = ticker_reports[\"Period Ending\"]\n",
    "    # print(report_date)\n",
    "    report_dates.append(report_date)\n",
    "    report_data.append(ticker_reports[\"Accounts Payable\":\"Treasury Stock\"]) # fundamentals csv has 2 more columns after Treasury Stock but not all rows have data\n",
    "\n",
    "  # Get ticker prices between dates of 2 reports\n",
    "  for period in range(len(report_dates) - 1):\n",
    "    price_start_date = report_dates[period]\n",
    "    price_end_date = report_dates[period + 1]\n",
    "    period_prices = []\n",
    "\n",
    "    period_prices_full = prices[(prices[\"symbol\"] == ticker) & (prices[\"date\"] > price_start_date) & (prices[\"date\"] <= price_end_date)][\"close\"] # chose to use closing price for each day, not sure if it matters\n",
    "    if len(period_prices_full) <= prediction_lead: # skip periods that don't have any price data before prediction lead\n",
    "      continue\n",
    "    step_size = (len(period_prices_full) - 1 - prediction_lead) / max(1, (price_count - 1)) # avg index distance between sampled prices (float for better distribution)\n",
    "\n",
    "    for count in range(price_count):\n",
    "      step = int(round(count * step_size)) # calculate index to sample from and cast to int\n",
    "      period_prices.append(period_prices_full.iloc[step])\n",
    "\n",
    "    # print(period_prices)\n",
    "\n",
    "    # Combine data into dataframe\n",
    "    fundamentals_start = report_data[period].tolist()\n",
    "    fundamentals_end = report_data[period + 1].tolist()\n",
    "    data_row = period_prices + fundamentals_start + fundamentals_end\n",
    "    feature_data.append(data_row)\n",
    "    label_data.append(period_prices_full.iloc[-1])\n",
    "\n",
    "# Make array of column names\n",
    "fundamentals_columns_start = fundamentals.add_suffix(\" (Start)\").columns.tolist()[3:76]\n",
    "fundamentals_columns_end = fundamentals.add_suffix(\" (End)\").columns.tolist()[3:76]\n",
    "price_columns = [f\"Price {i}\" for i in range(1, price_count + 1)]\n",
    "data_columns = price_columns + fundamentals_columns_start + fundamentals_columns_end + [\"Final Price\"]\n",
    "\n",
    "print(f\"Dataset: {len(label_data)} rows\")\n",
    "# Create dataframe from data and column names (not using, going straight from list to tensor instead)\n",
    "# df = pd.DataFrame(data_lst, columns=data_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aEx1AJbXdcC",
    "outputId": "7bfa4612-4e3b-4564-8d82-f493ef96e8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset = TensorDataset(feature_tensor, label_tensor)\\n# Split data\\ntrain_set, validation_set = train_test_split(dataset, test_size=0.2, random_state=42)\\nvalidation_set, test_set = train_test_split(validation_set, test_size=0.5, random_state=42)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset tensor\n",
    "feature_tensor = torch.tensor(np.array(feature_data), dtype=torch.float32)\n",
    "label_tensor = torch.tensor(np.array(label_data), dtype=torch.float32)\n",
    "\n",
    "# Note: Zach commented this out to normalize data without labels, did a different split in cell below\n",
    "'''\n",
    "dataset = TensorDataset(feature_tensor, label_tensor)\n",
    "# Split data\n",
    "train_set, validation_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "validation_set, test_set = train_test_split(validation_set, test_size=0.5, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8pNNaZE9eFG6"
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "mean = feature_tensor.mean(dim=0, keepdim=True)\n",
    "std = feature_tensor.std(dim=0, keepdim=True)\n",
    "normalized_features = (feature_tensor - mean) / std\n",
    "\n",
    "# Combine features and labels into a dataset\n",
    "dataset = TensorDataset(normalized_features, label_tensor)\n",
    "\n",
    "# Split into train, validation, and test\n",
    "dataset_size = len(feature_tensor)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.20 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors to files\n",
    "torch.save(train_dataset, \"train_set.pt\")\n",
    "torch.save(val_dataset, \"val_set.pt\")\n",
    "torch.save(test_dataset, \"test_set.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
